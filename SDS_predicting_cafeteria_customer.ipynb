{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "get_ipython().magic('matplotlib inline')\n",
    "from matplotlib import rc, font_manager\n",
    "rc('font',family='NanumSquareR')\n",
    "import os\n",
    "\n",
    "#making lag !\n",
    "dir_=# 데이터가 포함된 directory\n",
    "os.chdir(dir_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기상예측 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####기상 예측 모델만들기\n",
    "\n",
    "wewe=pd.read_csv(\"wether_6hour.csv\")\n",
    "wewe.head()\n",
    "wewe.tail(30)\n",
    "\n",
    "\n",
    "import datetime\n",
    "def dow(date):\n",
    "    conver_date=datetime.datetime.strptime(date,\"%Y-%m-%d\").date()\n",
    "\n",
    "    \n",
    "    days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    dayNumber=conver_date.weekday()\n",
    "    return days[dayNumber]\n",
    "\n",
    "\n",
    "wewe['yoil']=wewe['date'].map(dow)\n",
    "wewe.head()\n",
    "\n",
    "nd1=wewe.loc[wewe['date']<'2018-12-31',]\n",
    "nd2=wewe.loc[wewe['date']>='2018-12-31',]\n",
    "l1=nd1['date'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])\n",
    "l2=nd2['date'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])+52\n",
    "l3=l1.append(l2)\n",
    "l3\n",
    "wewe['week_num']=l3\n",
    "wewe.head(20)\n",
    "\n",
    "# for humidity\n",
    "wewe['h_monday']=np.repeat(0,len(wewe))\n",
    "wewe['h_tuesday']=np.repeat(0,len(wewe))\n",
    "wewe['h_wednesday']=np.repeat(0,len(wewe))\n",
    "wewe['h_thursday']=np.repeat(0,len(wewe))\n",
    "wewe['h_friday']=np.repeat(0,len(wewe)) \n",
    "\n",
    "\n",
    "for i in range(7,len(wewe),1):\n",
    "    #monday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),])==0:\n",
    "        wewe.loc[i,'h_monday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'h_monday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),]['humidity'])\n",
    "\n",
    "    #tuesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),])==0:\n",
    "        wewe.loc[i,'h_tuesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'h_tuesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),]['humidity'])\n",
    "\n",
    "    #wednesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),])==0:\n",
    "        wewe.loc[i,'h_wednesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'h_wednesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),]['humidity'])\n",
    "\n",
    "    #thursday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),])==0:\n",
    "        wewe.loc[i,'h_thursday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'h_thursday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),]['humidity'])\n",
    "\n",
    "    #friday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),])==0:\n",
    "        wewe.loc[i,'h_friday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'h_friday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),]['humidity'])\n",
    "\n",
    "\n",
    "\n",
    "wewe.head(20)\n",
    "# for rain\n",
    "\n",
    "wewe['r_monday']=np.repeat(0,len(wewe))\n",
    "wewe['r_tuesday']=np.repeat(0,len(wewe))\n",
    "wewe['r_wednesday']=np.repeat(0,len(wewe))\n",
    "wewe['r_thursday']=np.repeat(0,len(wewe))\n",
    "wewe['r_friday']=np.repeat(0,len(wewe)) \n",
    "\n",
    "\n",
    "for i in range(7,len(wewe),1):\n",
    "    #monday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),])==0:\n",
    "        wewe.loc[i,'r_monday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'r_monday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),]['rain'])\n",
    "\n",
    "    #tuesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),])==0:\n",
    "        wewe.loc[i,'r_tuesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'r_tuesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),]['rain'])\n",
    "\n",
    "    #wednesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),])==0:\n",
    "        wewe.loc[i,'r_wednesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'r_wednesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),]['rain'])\n",
    "\n",
    "    #thursday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),])==0:\n",
    "        wewe.loc[i,'r_thursday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'r_thursday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),]['rain'])\n",
    "\n",
    "    #friday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),])==0:\n",
    "        wewe.loc[i,'r_friday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'r_friday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),]['rain'])\n",
    "\n",
    "wewe.head()\n",
    "#for wind\n",
    "        \n",
    "wewe['w_monday']=np.repeat(0,len(wewe))\n",
    "wewe['w_tuesday']=np.repeat(0,len(wewe))\n",
    "wewe['w_wednesday']=np.repeat(0,len(wewe))\n",
    "wewe['w_thursday']=np.repeat(0,len(wewe))\n",
    "wewe['w_friday']=np.repeat(0,len(wewe)) \n",
    "\n",
    "\n",
    "for i in range(7,len(wewe),1):\n",
    "    #monday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),])==0:\n",
    "        wewe.loc[i,'w_monday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'w_monday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),]['wind'])\n",
    "\n",
    "    #tuesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),])==0:\n",
    "        wewe.loc[i,'w_tuesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'w_tuesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),]['wind'])\n",
    "\n",
    "    #wednesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),])==0:\n",
    "        wewe.loc[i,'w_wednesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'w_wednesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),]['wind'])\n",
    "\n",
    "    #thursday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),])==0:\n",
    "        wewe.loc[i,'w_thursday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'w_thursday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),]['wind'])\n",
    "\n",
    "    #friday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),])==0:\n",
    "        wewe.loc[i,'w_friday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'w_friday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),]['wind'])\n",
    "wewe.head()\n",
    "#for temp\n",
    "        \n",
    "wewe['t_monday']=np.repeat(0,len(wewe))\n",
    "wewe['t_tuesday']=np.repeat(0,len(wewe))\n",
    "wewe['t_wednesday']=np.repeat(0,len(wewe))\n",
    "wewe['t_thursday']=np.repeat(0,len(wewe))\n",
    "wewe['t_friday']=np.repeat(0,len(wewe)) \n",
    "\n",
    "\n",
    "for i in range(7,len(wewe),1):\n",
    "    #monday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),])==0:\n",
    "        wewe.loc[i,'t_monday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'t_monday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Monday'),]['temp'])\n",
    "\n",
    "    #tuesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),])==0:\n",
    "        wewe.loc[i,'t_tuesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'t_tuesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Tuesday'),]['temp'])\n",
    "\n",
    "    #wednesday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),])==0:\n",
    "        wewe.loc[i,'t_wednesday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'t_wednesday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Wednesday'),]['temp'])\n",
    "\n",
    "    #thursday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),])==0:\n",
    "        wewe.loc[i,'t_thursday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'t_thursday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Thursday'),]['temp'])\n",
    "\n",
    "    #friday\n",
    "    if len(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),])==0:\n",
    "        wewe.loc[i,'t_friday']=0\n",
    "    else:\n",
    "        wewe.loc[i,'t_friday']=float(wewe.loc[(wewe['week_num']==wewe.loc[i,'week_num']-1)&(wewe['yoil']=='Friday'),]['temp'])\n",
    "\n",
    "wewe['yoil']\n",
    "wewe.head(20)\n",
    "##revision (yoil, month, season)\n",
    "import datetime\n",
    "def dow(date):\n",
    "    conver_date=datetime.datetime.strptime(date,\"%Y-%m-%d\").date()\n",
    "\n",
    "    \n",
    "    days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    dayNumber=conver_date.weekday()\n",
    "    return days[dayNumber]\n",
    "\n",
    "\n",
    "wewe['month']=wewe['date'].map(lambda x : int(x[5:7]))\n",
    "wewe['winter']=(wewe['month']==12)+(wewe['month']==1)+(wewe['month']==2)\n",
    "wewe['summer']=(wewe['month']==6)+(wewe['month']==7)+(wewe['month']==8)\n",
    "wewe['month']\n",
    "\n",
    "dummy1=pd.get_dummies(wewe['yoil'], prefix='C1', drop_first=True)\n",
    "dummy2=pd.get_dummies(wewe['month'], prefix='D1', drop_first=True)\n",
    "dummy3=pd.get_dummies(wewe['winter'], prefix='E1', drop_first=True)\n",
    "dummy4=pd.get_dummies(wewe['summer'], prefix='F1', drop_first=True)\n",
    "\n",
    "\n",
    "wewe=pd.concat([wewe,dummy1,dummy2,dummy3,dummy4],axis=1)\n",
    "\n",
    "wewe=wewe.drop(['yoil','month','summer','winter'],axis=1)\n",
    "\n",
    "wewe.head()\n",
    "###\n",
    "wewe.columns\n",
    "hh=wewe[['date','humidity','h_monday','h_tuesday','h_wednesday','h_thursday','h_friday','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','C1_Saturday','C1_Sunday']]\n",
    "tt=wewe[['date','temp','t_monday','t_tuesday','t_wednesday','t_thursday','t_friday','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','C1_Saturday','C1_Sunday']]\n",
    "ww=wewe[['date','wind','w_monday','w_tuesday','w_wednesday','w_thursday','w_friday','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','C1_Saturday','C1_Sunday']]\n",
    "rr=wewe[['date','rain','r_monday','r_tuesday','r_wednesday','r_thursday','r_friday','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','C1_Saturday','C1_Sunday']]\n",
    "\n",
    "hh.head()\n",
    "\n",
    "\n",
    "#divide train with test(기상)\n",
    "#humidity\n",
    "date_list=[]\n",
    "humidity_list=[]\n",
    "temperature_list=[]\n",
    "windspeed_list=[]\n",
    "rain_list=[]\n",
    "\n",
    "hh['date']\n",
    "str(pd.to_datetime('2019-05-20')+pd.DateOffset(days=7*1))[:10]\n",
    "\n",
    "\n",
    "for i in range(0,31,1): #changeable\n",
    "    datename1=str(pd.to_datetime('2018-12-31')+pd.DateOffset(days=7*i))[:10]\n",
    "    datename2=str(pd.to_datetime('2018-12-31')+pd.DateOffset(days=7*(i+1)))[:10]\n",
    "    \n",
    "    train=hh.loc[hh['date']<datename1,]\n",
    "    test=hh.loc[hh['date']>=datename1,]\n",
    "    test=test.loc[test['date']<datename2,]\n",
    "    #modeling1 (random forest)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    y_train=train['humidity']\n",
    "    X_train=train.drop(['humidity','date'],axis=1)\n",
    "    \n",
    "    y_test=test['humidity']\n",
    "    X_test=test.drop(['humidity','date'],axis=1)\n",
    "    \n",
    "    len(X_test.columns)\n",
    "    rf=RandomForestRegressor(n_estimators=1000,criterion='mse',n_jobs=3)\n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    pred_h=rf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #temperature\n",
    "    train=tt.loc[tt['date']<datename1,]\n",
    "    test=tt.loc[tt['date']>=datename1,]\n",
    "    test=test.loc[test['date']<datename2,]\n",
    "    #modeling1 (random forest)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    y_train=train['temp']\n",
    "    X_train=train.drop(['temp','date'],axis=1)\n",
    "    \n",
    "    y_test=test['temp']\n",
    "    X_test=test.drop(['temp','date'],axis=1)\n",
    "    \n",
    "    len(X_test.columns)\n",
    "    rf=RandomForestRegressor(n_estimators=1000,criterion='mse',n_jobs=3)\n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    pred_t=rf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #wind\n",
    "    train=ww.loc[ww['date']<datename1,]\n",
    "    test=ww.loc[ww['date']>=datename1,]\n",
    "    test=test.loc[test['date']<datename2,]\n",
    "    #modeling1 (random forest)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    y_train=train['wind']\n",
    "    X_train=train.drop(['wind','date'],axis=1)\n",
    "    \n",
    "    y_test=test['wind']\n",
    "    X_test=test.drop(['wind','date'],axis=1)\n",
    "    \n",
    "    len(X_test.columns)\n",
    "    rf=RandomForestRegressor(n_estimators=1000,criterion='mse',n_jobs=3)\n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    pred_w=rf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #rain\n",
    "    train=rr.loc[rr['date']<datename1,]\n",
    "    test=rr.loc[rr['date']>=datename1,]\n",
    "    test=test.loc[test['date']<datename2,]\n",
    "    #modeling1 (random forest)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    y_train=train['rain']\n",
    "    X_train=train.drop(['rain','date'],axis=1)\n",
    "    \n",
    "    y_test=test['rain']\n",
    "    X_test=test.drop(['rain','date'],axis=1)\n",
    "    \n",
    "    len(X_test.columns)\n",
    "    rf=RandomForestRegressor(n_estimators=1000,criterion='mse',n_jobs=3)\n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    pred_r=rf.predict(X_test)\n",
    "    \n",
    "    test2=hh.loc[hh['date']>=datename1,]\n",
    "    test2=test.loc[test['date']<datename2,]\n",
    "    pred_d=list(test2['date'])\n",
    "    \n",
    "    date_list=date_list+pred_d\n",
    "    humidity_list=humidity_list+list(pred_h)\n",
    "    temperature_list=temperature_list+list(pred_t)\n",
    "    windspeed_list=windspeed_list+list(pred_w)\n",
    "    rain_list=rain_list+list(pred_r)\n",
    "\n",
    "\n",
    "df_total2.head()\n",
    "df_total2.tail()\n",
    "#제일마지막에...\n",
    "df_total2=pd.DataFrame({'SELL_DATE':date_list,'humidity':humidity_list,'temp':temperature_list,'wind':windspeed_list,'rain':rain_list})\n",
    "\n",
    "df_total2.to_csv(dir_+\"\\\\wether_predict.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "get_ipython().magic('matplotlib inline')\n",
    "from matplotlib import rc, font_manager\n",
    "rc('font',family='NanumSquareR')\n",
    "import os\n",
    "\n",
    "weather_predict=pd.read_csv(\"wether_predict.csv\",engine='python')\n",
    "\n",
    "df1=pd.read_csv(\"total_brand1.csv\")\n",
    "df2=pd.read_csv(\"total_brand2.csv\")\n",
    "df3=pd.read_csv(\"total_brand3.csv\")\n",
    "df4=pd.read_csv(\"total_brand4.csv\")\n",
    "df5=pd.read_csv(\"total_brand5.csv\")\n",
    "df6=pd.read_csv(\"total_brand6.csv\")\n",
    "df7=pd.read_csv(\"total_brand7.csv\")\n",
    "df8=pd.read_csv(\"total_brand8.csv\")\n",
    "df9=pd.read_csv(\"total_brand9.csv\")\n",
    "df10=pd.read_csv(\"total_brand10.csv\")\n",
    "df11=pd.read_csv(\"total_brand11.csv\")\n",
    "df12=pd.read_csv(\"total_brand12.csv\")\n",
    "df13=pd.read_csv(\"total_brand13.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "df_total=pd.DataFrame({'month':[5,4,3,2,1]})\n",
    "\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "list6=[]\n",
    "list7=[]\n",
    "list8=[]\n",
    "list9=[]\n",
    "list10=[]\n",
    "list11=[]\n",
    "list12=[]\n",
    "list13=[]\n",
    "\n",
    "\n",
    "\n",
    "total_list=[list1,list2,list3,list4,list5,list6,list7,list8,list9,list10,list11,list12,list13]\n",
    "\n",
    "\n",
    "for i in range(13):\n",
    "    new_brand1=df_list[i]\n",
    "    new_brand1=new_brand1.fillna(0)\n",
    "    new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "    new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "    new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "    \n",
    "    holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "    holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31'])\n",
    "    holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "    \n",
    "    new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "    new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "    new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "    \n",
    "    idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "    dfdf=new_brand1.drop(idx)\n",
    "    new_brand1=dfdf\n",
    "    \n",
    "    \n",
    "    #modeling\n",
    "    \n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "    \n",
    "    new_brand1.head()\n",
    "    #divide train with test\n",
    "    \n",
    "    for j in range(0,21,1):\n",
    "    \n",
    "        datename1=str(pd.to_datetime('2018-12-31')+pd.DateOffset(days=7*j))[:10]\n",
    "        datename2=str(pd.to_datetime('2018-12-31')+pd.DateOffset(days=7*(j+1)))[:10]\n",
    "    \n",
    "        train=new_brand1.loc[new_brand1['SELL_DATE']<datename1,]\n",
    "        test=new_brand1.loc[new_brand1['SELL_DATE']>=datename1,]\n",
    "        test=test.loc[test['SELL_DATE']<datename2,]\n",
    "        \n",
    "        #modeling1 (random forest)\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        \n",
    "        y_train=train['y']\n",
    "        X_train=train.drop(['y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "        X_train.columns\n",
    "        \n",
    "        y_test=test['y']\n",
    "        X_test=test.drop(['y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "        \n",
    "        len(X_test.columns)\n",
    "        rf=RandomForestRegressor(n_estimators=1000,criterion='mae',n_jobs=3)\n",
    "        rf.fit(X_train,y_train)\n",
    "        \n",
    "        X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "        X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "        X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "        X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "        \n",
    "        pred=rf.predict(X_test4)\n",
    "        \n",
    "        real=test['y']\n",
    "        \n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        mae55=mean_absolute_error(real,pred) #MAE\n",
    "        total_list[i].append(mae55)\n",
    "\n",
    "brand1=pd.Series(total_list[0])\n",
    "brand2=pd.Series(total_list[1])\n",
    "brand3=pd.Series(total_list[2])\n",
    "brand4=pd.Series(total_list[3])\n",
    "brand5=pd.Series(total_list[4])\n",
    "brand6=pd.Series(total_list[5])\n",
    "brand7=pd.Series(total_list[6])\n",
    "brand8=pd.Series(total_list[7])\n",
    "brand9=pd.Series(total_list[8])\n",
    "brand10=pd.Series(total_list[9])\n",
    "brand11=pd.Series(total_list[10])\n",
    "brand12=pd.Series(total_list[11])\n",
    "brand13=pd.Series(total_list[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 1, 5  예측\n",
    "\n",
    "기본+기상4(예측) 배깅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_predict=pd.read_csv(\"wether_predict.csv\",engine='python')\n",
    "weather_predict.tail()\n",
    "\n",
    "df1=pd.read_csv(\"total_brand1.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"total_brand2.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"total_brand3.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"total_brand4.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"total_brand5.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"total_brand6.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"total_brand7.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"total_brand8.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"total_brand9.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"total_brand10.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"total_brand11.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"total_brand12.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"total_brand13.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=6\n",
    "new_brand1=df_list[i]\n",
    "new_brand1=new_brand1.fillna(0)\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12', 'C1_Saturday', 'C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "else:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12', 'C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "\n",
    "new_brand1['y_g12']=new_brand1['y_g1']+new_brand1['y_g2']\n",
    "new_brand1['y_g34']=new_brand1['y_g3']+new_brand1['y_g4']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train1=train['y_g12']\n",
    "X_train=train.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train1[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train1,X_train)\n",
    "results = model.fit()\n",
    "pred21=results.predict(X_test) # for y_g12\n",
    "\n",
    "y_train2=train['y_g34']\n",
    "X_train=train.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train2[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train2,X_train)\n",
    "results = model.fit()\n",
    "pred22=results.predict(X_test) # for y_g34    \n",
    "\n",
    "pred2=pred21+pred22\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 2 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.기본+기상4(예측) 배깅\n",
    "\n",
    "weather_predict=pd.read_csv(\"wether_predict.csv\",engine='python')\n",
    "\n",
    "df1=pd.read_csv(\"total_brand1.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"total_brand2.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"total_brand3.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"total_brand4.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"total_brand5.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"total_brand6.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"total_brand7.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"total_brand8.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"total_brand9.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"total_brand10.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"total_brand11.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"total_brand12.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"total_brand13.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "list6=[]\n",
    "list7=[]\n",
    "list8=[]\n",
    "list9=[]\n",
    "list10=[]\n",
    "list11=[]\n",
    "list12=[]\n",
    "list13=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=6\n",
    "new_brand1=df_list[i]\n",
    "new_brand1=new_brand1.fillna(0)\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12', 'C1_Saturday' ,'C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "else:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "\n",
    "new_brand1['y_g12']=new_brand1['y_g1']+new_brand1['y_g2']\n",
    "new_brand1['y_g34']=new_brand1['y_g3']+new_brand1['y_g4']\n",
    "\n",
    "\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train1=train['y_g12']\n",
    "y_train2=train['y_g34']\n",
    "X_train=train.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "\n",
    "pppp=len(X_train.columns)\n",
    "rf=RandomForestRegressor(n_estimators=1000,criterion='mae',n_jobs=3,max_features=pppp)\n",
    "rf.fit(X_train,y_train1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "pred1=rf.predict(X_test4)\n",
    "\n",
    "pppp=len(X_train.columns)\n",
    "rf=RandomForestRegressor(n_estimators=1000,criterion='mae',n_jobs=3,max_features=pppp)\n",
    "rf.fit(X_train,y_train2)\n",
    "\n",
    "pred2=rf.predict(X_test4)    \n",
    "\n",
    "pred=pred1+pred2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 식당최종제출(브랜드3,6,7,11) 예측\n",
    "- 식자재 데이터 배깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.식자재, 배깅\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "get_ipython().magic('matplotlib inline')\n",
    "from matplotlib import rc, font_manager\n",
    "rc('font',family='NanumSquareR')\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\admin\\\\Dropbox\\\\sds공모전\\\\민수_모델평가\")\n",
    "meal1_init=pd.read_csv(\"mealData_meal.csv\")\n",
    "meal2_init=pd.read_csv(\"mealData_customer.csv\")\n",
    "meal1_add=pd.read_csv(\"mealData_meal_0525_0731.csv\")\n",
    "meal2_add=pd.read_csv(\"mealData_customer_0525_0731.csv\")\n",
    "\n",
    "##식자재 래그 (1주일) 만드는 법################################################\n",
    "material=pd.read_csv(\"material3.csv\",encoding='euc-kr')\n",
    "material.head()\n",
    "import datetime\n",
    "def dow(date):\n",
    "    conver_date=datetime.datetime.strptime(date,\"%Y-%m-%d\").date()\n",
    "\n",
    "    \n",
    "    days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    dayNumber=conver_date.weekday()\n",
    "    return days[dayNumber]\n",
    "\n",
    "\n",
    "material['yoil']=material['SELL_DATE'].map(dow)\n",
    "material.head()\n",
    "\n",
    "nd1=material.loc[material['SELL_DATE']<'2018-12-31',]\n",
    "nd2=material.loc[material['SELL_DATE']>='2018-12-31',]\n",
    "l1=nd1['SELL_DATE'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])\n",
    "l2=nd2['SELL_DATE'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])+52\n",
    "l3=l1.append(l2)\n",
    "l3\n",
    "material['week_num']=l3+2\n",
    "material.tail(20)\n",
    "material.loc[[0,1,2,3,4],'week_num']=1\n",
    "material.loc[[5,6,7,8],'week_num']=2\n",
    "material.columns\n",
    "material_list=['갈치_price', '감자_price', '계란_price', '고등어_price',\n",
    "       '고춧가루_price', '꽁치_price', '닭고기_price', '당근_price', '대파_price',\n",
    "       '마늘_price', '멸치_price', '목살_price', '무_price', '배_price', '배추_price',\n",
    "       '상추_price', '새우_price', '쌀_price', '양파_price', '우유_price',\n",
    "       '참깨_price',  '팽이버섯_price', '한우양지_price', '호박_price',\n",
    "       '고구마_price', '시금치_price', '오이_price', '오징어_price', '미나리_price',\n",
    "       '깻잎_price', '피망_price']\n",
    "\n",
    "for k in range(31):\n",
    "    material[material_list[k]+'_lag1']=np.repeat(0,len(material))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(9,len(material),1):\n",
    "    for k in range(31):\n",
    "        material.loc[i,material_list[k]+'_lag1']=float(material.loc[material['week_num']==material.loc[i,'week_num']-1,][material_list[k]].mean())\n",
    "    \n",
    "material.head()\n",
    "material=material[[material_list[k]+'_lag1' for k in range(31)]+['SELL_DATE']]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "check_idx=meal2_add['CUSTOMER_ID'].map(lambda x: x in list(meal2_init['CUSTOMER_ID']))\n",
    "check_idx.sum()\n",
    "\n",
    "meal2_add=meal2_add.loc[~check_idx,]\n",
    "\n",
    "meal2=pd.concat([meal2_init,meal2_add])\n",
    "meal1=pd.concat([meal1_init,meal1_add])\n",
    "\n",
    "meal1=meal1.reset_index()\n",
    "meal1=meal1.drop(\"index\",axis=1)\n",
    "\n",
    "meal2=meal2.reset_index()\n",
    "meal2=meal2.drop(\"index\",axis=1)\n",
    "\n",
    "a1=meal1\n",
    "a2=meal2\n",
    "\n",
    "a1.tail(70)\n",
    "a2.head(50)\n",
    "a3=a1.groupby('CUSTOMER_ID')['QUANTITY'].sum()\n",
    "a3=a3.reset_index()\n",
    "a3.head()\n",
    "dangol_id=list(a3.loc[a3['QUANTITY']>=300,]['CUSTOMER_ID'])\n",
    "\n",
    "dangol_idx=a1['CUSTOMER_ID'].map(lambda x : x in dangol_id)\n",
    "dangol=a1.loc[dangol_idx,]\n",
    "non_dangol=a1.loc[~dangol_idx,]\n",
    "\n",
    "dangol.head()\n",
    "non_dangol.head()\n",
    "125343+898304\n",
    "##\n",
    "df1=pd.read_csv(\"new_brand1_6hour.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"new_brand2_6hour.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"new_brand3_6hour.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"new_brand4_6hour.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"new_brand5_6hour.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"new_brand6_6hour.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"new_brand7_6hour.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"new_brand8_6hour.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"new_brand9_6hour.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"new_brand10_6hour.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"new_brand11_6hour.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"new_brand12_6hour.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"new_brand13_6hour.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "df_total=pd.DataFrame({'month':[7,6,5,4,3,2]})\n",
    "\n",
    "\n",
    "i=8\n",
    "    \n",
    "\n",
    "if i ==0:\n",
    "    brandname=\"Chef`sCounter\"\n",
    "elif i==1:\n",
    "    brandname=\"우리미각면\"\n",
    "elif i==2:\n",
    "    brandname=\"나폴리폴리\"\n",
    "elif i==3:\n",
    "    brandname=\"TakeOut\"\n",
    "elif i==4:\n",
    "    brandname=\"KOREAN1\"\n",
    "elif i==5:\n",
    "    brandname=\"싱푸차이나\"\n",
    "elif i==6:\n",
    "    brandname=\"KOREAN2\"\n",
    "elif i==7:\n",
    "    brandname=\"Western\"\n",
    "elif i==8:\n",
    "    brandname=\"고슬고슬비빈\"\n",
    "elif i==9:\n",
    "    brandname=\"아시안픽스\"\n",
    "elif i==10:\n",
    "    brandname=\"스냅스낵\"\n",
    "elif i==11:\n",
    "    brandname=\"가츠엔\"\n",
    "else:\n",
    "    brandname=\"탕맛기픈\"\n",
    "    \n",
    "c1=dangol.loc[dangol['BRAND']==brandname,]\n",
    "y_dangol=c1.groupby('SELL_DATE')['QUANTITY'].sum()\n",
    "y_dangol=y_dangol.reset_index()\n",
    "y_dangol.columns=['SELL_DATE','y_dangol']\n",
    "\n",
    "\n",
    "c2=non_dangol.loc[non_dangol['BRAND']==brandname,]\n",
    "y_non_dangol=c2.groupby('SELL_DATE')['QUANTITY'].sum()\n",
    "y_non_dangol=y_non_dangol.reset_index()\n",
    "y_non_dangol.columns=['SELL_DATE','y_non_dangol']\n",
    "\n",
    "\n",
    "new_brand1=df_list[i]\n",
    "#new_brand1=new_brand1.fillna(0)\n",
    "#new_brand1['y_g12']=new_brand1['y_g1']+new_brand1['y_g2']\n",
    "#new_brand1['y_g34']=new_brand1['y_g3']+new_brand1['y_g4']\n",
    "new_brand1.columns.values[0]='SELL_DATE'\n",
    "new_brand1.columns.values[len(new_brand1.columns)-1]='y'\n",
    "\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31','2019-06-07'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "new_brand1=pd.merge(new_brand1,material,how='left',on='SELL_DATE')\n",
    "new_brand1=pd.merge(new_brand1,y_dangol,how='left',on='SELL_DATE')\n",
    "new_brand1=pd.merge(new_brand1,y_non_dangol,how='left',on='SELL_DATE')\n",
    "\n",
    "notnan_idx=set(new_brand1.loc[new_brand1['감자_price_lag1']>0,].index)\n",
    "nan_idx=list(set(new_brand1.index) - notnan_idx)\n",
    "\n",
    "for j in nan_idx:\n",
    "    for k in range(31):\n",
    "        new_brand1.loc[j,material_list[k]+'_lag1']=new_brand1.loc[j-1,material_list[k]+'_lag1']\n",
    "\n",
    "\n",
    "new_brand1.columns\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "weather_predict=pd.read_csv(\"날씨예측결과.csv\",engine='python')\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['SELL_DATE', 'humidity', 'rain', 'temp', 'wind', 'C1_Monday', 'C1_Saturday',\n",
    "       'C1_Thursday', 'C1_Tuesday', 'C1_Wednesday', 'D1_2', 'D1_3', 'D1_4',\n",
    "       'D1_5', 'D1_6', 'D1_7', 'D1_8', 'D1_9', 'D1_10', 'D1_11', 'D1_12',\n",
    "       'E1_True', 'F1_True', 'y', 'holiday1', 'holiday2', 'holiday3',\n",
    "       '갈치_price_lag1', '감자_price_lag1', '계란_price_lag1', '고등어_price_lag1',\n",
    "       '고춧가루_price_lag1', '꽁치_price_lag1', '닭고기_price_lag1', '당근_price_lag1',\n",
    "       '대파_price_lag1', '마늘_price_lag1', '멸치_price_lag1', '목살_price_lag1',\n",
    "       '무_price_lag1', '배_price_lag1', '배추_price_lag1', '상추_price_lag1',\n",
    "       '새우_price_lag1', '쌀_price_lag1', '양파_price_lag1', '우유_price_lag1',\n",
    "       '참깨_price_lag1', '팽이버섯_price_lag1', '한우양지_price_lag1', '호박_price_lag1',\n",
    "       '고구마_price_lag1', '시금치_price_lag1', '오이_price_lag1', '오징어_price_lag1',\n",
    "       '미나리_price_lag1', '깻잎_price_lag1', '피망_price_lag1', 'y_dangol',\n",
    "       'y_non_dangol']]\n",
    "\n",
    "else:\n",
    "    new_brand1=new_brand1[['SELL_DATE', 'humidity', 'rain', 'temp', 'wind', 'C1_Monday',\n",
    "       'C1_Thursday', 'C1_Tuesday', 'C1_Wednesday', 'D1_2', 'D1_3', 'D1_4',\n",
    "       'D1_5', 'D1_6', 'D1_7', 'D1_8', 'D1_9', 'D1_10', 'D1_11', 'D1_12',\n",
    "       'E1_True', 'F1_True', 'y', 'holiday1', 'holiday2', 'holiday3',\n",
    "       '갈치_price_lag1', '감자_price_lag1', '계란_price_lag1', '고등어_price_lag1',\n",
    "       '고춧가루_price_lag1', '꽁치_price_lag1', '닭고기_price_lag1', '당근_price_lag1',\n",
    "       '대파_price_lag1', '마늘_price_lag1', '멸치_price_lag1', '목살_price_lag1',\n",
    "       '무_price_lag1', '배_price_lag1', '배추_price_lag1', '상추_price_lag1',\n",
    "       '새우_price_lag1', '쌀_price_lag1', '양파_price_lag1', '우유_price_lag1',\n",
    "       '참깨_price_lag1', '팽이버섯_price_lag1', '한우양지_price_lag1', '호박_price_lag1',\n",
    "       '고구마_price_lag1', '시금치_price_lag1', '오이_price_lag1', '오징어_price_lag1',\n",
    "       '미나리_price_lag1', '깻잎_price_lag1', '피망_price_lag1', 'y_dangol',\n",
    "       'y_non_dangol']]\n",
    "\n",
    "\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train=train['y']\n",
    "X_train=train.drop(['y','SELL_DATE','y_dangol','y_non_dangol'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y','SELL_DATE','y_dangol','y_non_dangol'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train,X_train)\n",
    "results = model.fit()\n",
    "pred2=results.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 4,8 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal1_init=pd.read_csv(\"mealData_meal.csv\")\n",
    "meal2_init=pd.read_csv(\"mealData_customer.csv\")\n",
    "meal1_add=pd.read_csv(\"mealData_meal_0525_0731.csv\")\n",
    "meal2_add=pd.read_csv(\"mealData_customer_0525_0731.csv\")\n",
    "\n",
    "##식자재 래그 (1주일) 만드는 법################################################\n",
    "material=pd.read_csv(\"material3.csv\",encoding='euc-kr')\n",
    "material.head()\n",
    "import datetime\n",
    "def dow(date):\n",
    "    conver_date=datetime.datetime.strptime(date,\"%Y-%m-%d\").date()\n",
    "\n",
    "    \n",
    "    days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    dayNumber=conver_date.weekday()\n",
    "    return days[dayNumber]\n",
    "\n",
    "\n",
    "material['yoil']=material['SELL_DATE'].map(dow)\n",
    "material.head()\n",
    "\n",
    "nd1=material.loc[material['SELL_DATE']<'2018-12-31',]\n",
    "nd2=material.loc[material['SELL_DATE']>='2018-12-31',]\n",
    "l1=nd1['SELL_DATE'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])\n",
    "l2=nd2['SELL_DATE'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])+52\n",
    "l3=l1.append(l2)\n",
    "l3\n",
    "material['week_num']=l3+2\n",
    "material.tail(20)\n",
    "material.loc[[0,1,2,3,4],'week_num']=1\n",
    "material.loc[[5,6,7,8],'week_num']=2\n",
    "material.columns\n",
    "material_list=['갈치_price', '감자_price', '계란_price', '고등어_price',\n",
    "       '고춧가루_price', '꽁치_price', '닭고기_price', '당근_price', '대파_price',\n",
    "       '마늘_price', '멸치_price', '목살_price', '무_price', '배_price', '배추_price',\n",
    "       '상추_price', '새우_price', '쌀_price', '양파_price', '우유_price',\n",
    "       '참깨_price',  '팽이버섯_price', '한우양지_price', '호박_price',\n",
    "       '고구마_price', '시금치_price', '오이_price', '오징어_price', '미나리_price',\n",
    "       '깻잎_price', '피망_price']\n",
    "\n",
    "for k in range(31):\n",
    "    material[material_list[k]+'_lag1']=np.repeat(0,len(material))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(9,len(material),1):\n",
    "    for k in range(31):\n",
    "        material.loc[i,material_list[k]+'_lag1']=float(material.loc[material['week_num']==material.loc[i,'week_num']-1,][material_list[k]].mean())\n",
    "    \n",
    "material.head()\n",
    "material=material[[material_list[k]+'_lag1' for k in range(31)]+['SELL_DATE']]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "check_idx=meal2_add['CUSTOMER_ID'].map(lambda x: x in list(meal2_init['CUSTOMER_ID']))\n",
    "check_idx.sum()\n",
    "\n",
    "meal2_add=meal2_add.loc[~check_idx,]\n",
    "\n",
    "meal2=pd.concat([meal2_init,meal2_add])\n",
    "meal1=pd.concat([meal1_init,meal1_add])\n",
    "\n",
    "meal1=meal1.reset_index()\n",
    "meal1=meal1.drop(\"index\",axis=1)\n",
    "\n",
    "meal2=meal2.reset_index()\n",
    "meal2=meal2.drop(\"index\",axis=1)\n",
    "\n",
    "a1=meal1\n",
    "a2=meal2\n",
    "\n",
    "a1.tail(70)\n",
    "a2.head(50)\n",
    "a3=a1.groupby('CUSTOMER_ID')['QUANTITY'].sum()\n",
    "a3=a3.reset_index()\n",
    "a3.head()\n",
    "dangol_id=list(a3.loc[a3['QUANTITY']>=300,]['CUSTOMER_ID'])\n",
    "\n",
    "dangol_idx=a1['CUSTOMER_ID'].map(lambda x : x in dangol_id)\n",
    "dangol=a1.loc[dangol_idx,]\n",
    "non_dangol=a1.loc[~dangol_idx,]\n",
    "\n",
    "dangol.head()\n",
    "non_dangol.head()\n",
    "125343+898304\n",
    "##\n",
    "df1=pd.read_csv(\"new_brand1_6hour.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"new_brand2_6hour.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"new_brand3_6hour.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"new_brand4_6hour.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"new_brand5_6hour.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"new_brand6_6hour.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"new_brand7_6hour.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"new_brand8_6hour.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"new_brand9_6hour.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"new_brand10_6hour.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"new_brand11_6hour.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"new_brand12_6hour.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"new_brand13_6hour.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "df_total=pd.DataFrame({'month':[7,6,5,4,3,2]})\n",
    "\n",
    "\n",
    "i=8\n",
    "    \n",
    "\n",
    "if i ==0:\n",
    "    brandname=\"Chef`sCounter\"\n",
    "elif i==1:\n",
    "    brandname=\"우리미각면\"\n",
    "elif i==2:\n",
    "    brandname=\"나폴리폴리\"\n",
    "elif i==3:\n",
    "    brandname=\"TakeOut\"\n",
    "elif i==4:\n",
    "    brandname=\"KOREAN1\"\n",
    "elif i==5:\n",
    "    brandname=\"싱푸차이나\"\n",
    "elif i==6:\n",
    "    brandname=\"KOREAN2\"\n",
    "elif i==7:\n",
    "    brandname=\"Western\"\n",
    "elif i==8:\n",
    "    brandname=\"고슬고슬비빈\"\n",
    "elif i==9:\n",
    "    brandname=\"아시안픽스\"\n",
    "elif i==10:\n",
    "    brandname=\"스냅스낵\"\n",
    "elif i==11:\n",
    "    brandname=\"가츠엔\"\n",
    "else:\n",
    "    brandname=\"탕맛기픈\"\n",
    "    \n",
    "c1=dangol.loc[dangol['BRAND']==brandname,]\n",
    "y_dangol=c1.groupby('SELL_DATE')['QUANTITY'].sum()\n",
    "y_dangol=y_dangol.reset_index()\n",
    "y_dangol.columns=['SELL_DATE','y_dangol']\n",
    "\n",
    "\n",
    "c2=non_dangol.loc[non_dangol['BRAND']==brandname,]\n",
    "y_non_dangol=c2.groupby('SELL_DATE')['QUANTITY'].sum()\n",
    "y_non_dangol=y_non_dangol.reset_index()\n",
    "y_non_dangol.columns=['SELL_DATE','y_non_dangol']\n",
    "\n",
    "\n",
    "new_brand1=df_list[i]\n",
    "#new_brand1=new_brand1.fillna(0)\n",
    "#new_brand1['y_g12']=new_brand1['y_g1']+new_brand1['y_g2']\n",
    "#new_brand1['y_g34']=new_brand1['y_g3']+new_brand1['y_g4']\n",
    "new_brand1.columns.values[0]='SELL_DATE'\n",
    "new_brand1.columns.values[len(new_brand1.columns)-1]='y'\n",
    "\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31','2019-06-07'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "new_brand1=pd.merge(new_brand1,material,how='left',on='SELL_DATE')\n",
    "new_brand1=pd.merge(new_brand1,y_dangol,how='left',on='SELL_DATE')\n",
    "new_brand1=pd.merge(new_brand1,y_non_dangol,how='left',on='SELL_DATE')\n",
    "\n",
    "notnan_idx=set(new_brand1.loc[new_brand1['감자_price_lag1']>0,].index)\n",
    "nan_idx=list(set(new_brand1.index) - notnan_idx)\n",
    "\n",
    "for j in nan_idx:\n",
    "    for k in range(31):\n",
    "        new_brand1.loc[j,material_list[k]+'_lag1']=new_brand1.loc[j-1,material_list[k]+'_lag1']\n",
    "\n",
    "\n",
    "new_brand1.columns\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "weather_predict=pd.read_csv(\"날씨예측결과.csv\",engine='python')\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['SELL_DATE', 'humidity', 'rain', 'temp', 'wind', 'C1_Monday', 'C1_Saturday',\n",
    "       'C1_Thursday', 'C1_Tuesday', 'C1_Wednesday', 'D1_2', 'D1_3', 'D1_4',\n",
    "       'D1_5', 'D1_6', 'D1_7', 'D1_8', 'D1_9', 'D1_10', 'D1_11', 'D1_12',\n",
    "       'E1_True', 'F1_True', 'y', 'holiday1', 'holiday2', 'holiday3',\n",
    "       '갈치_price_lag1', '감자_price_lag1', '계란_price_lag1', '고등어_price_lag1',\n",
    "       '고춧가루_price_lag1', '꽁치_price_lag1', '닭고기_price_lag1', '당근_price_lag1',\n",
    "       '대파_price_lag1', '마늘_price_lag1', '멸치_price_lag1', '목살_price_lag1',\n",
    "       '무_price_lag1', '배_price_lag1', '배추_price_lag1', '상추_price_lag1',\n",
    "       '새우_price_lag1', '쌀_price_lag1', '양파_price_lag1', '우유_price_lag1',\n",
    "       '참깨_price_lag1', '팽이버섯_price_lag1', '한우양지_price_lag1', '호박_price_lag1',\n",
    "       '고구마_price_lag1', '시금치_price_lag1', '오이_price_lag1', '오징어_price_lag1',\n",
    "       '미나리_price_lag1', '깻잎_price_lag1', '피망_price_lag1', 'y_dangol',\n",
    "       'y_non_dangol']]\n",
    "\n",
    "else:\n",
    "    new_brand1=new_brand1[['SELL_DATE', 'humidity', 'rain', 'temp', 'wind', 'C1_Monday',\n",
    "       'C1_Thursday', 'C1_Tuesday', 'C1_Wednesday', 'D1_2', 'D1_3', 'D1_4',\n",
    "       'D1_5', 'D1_6', 'D1_7', 'D1_8', 'D1_9', 'D1_10', 'D1_11', 'D1_12',\n",
    "       'E1_True', 'F1_True', 'y', 'holiday1', 'holiday2', 'holiday3',\n",
    "       '갈치_price_lag1', '감자_price_lag1', '계란_price_lag1', '고등어_price_lag1',\n",
    "       '고춧가루_price_lag1', '꽁치_price_lag1', '닭고기_price_lag1', '당근_price_lag1',\n",
    "       '대파_price_lag1', '마늘_price_lag1', '멸치_price_lag1', '목살_price_lag1',\n",
    "       '무_price_lag1', '배_price_lag1', '배추_price_lag1', '상추_price_lag1',\n",
    "       '새우_price_lag1', '쌀_price_lag1', '양파_price_lag1', '우유_price_lag1',\n",
    "       '참깨_price_lag1', '팽이버섯_price_lag1', '한우양지_price_lag1', '호박_price_lag1',\n",
    "       '고구마_price_lag1', '시금치_price_lag1', '오이_price_lag1', '오징어_price_lag1',\n",
    "       '미나리_price_lag1', '깻잎_price_lag1', '피망_price_lag1', 'y_dangol',\n",
    "       'y_non_dangol']]\n",
    "\n",
    "\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train=train['y']\n",
    "X_train=train.drop(['y','SELL_DATE','y_dangol','y_non_dangol'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y','SELL_DATE','y_dangol','y_non_dangol'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train,X_train)\n",
    "results = model.fit()\n",
    "pred2=results.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 9 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal1_init=pd.read_csv(\"mealData_meal.csv\")\n",
    "meal2_init=pd.read_csv(\"mealData_customer.csv\")\n",
    "meal1_add=pd.read_csv(\"mealData_meal_0525_0731.csv\")\n",
    "meal2_add=pd.read_csv(\"mealData_customer_0525_0731.csv\")\n",
    "\n",
    "##식자재 래그 (1주일) 만드는 법################################################\n",
    "material=pd.read_csv(\"material3.csv\",encoding='euc-kr')\n",
    "material.head()\n",
    "import datetime\n",
    "def dow(date):\n",
    "    conver_date=datetime.datetime.strptime(date,\"%Y-%m-%d\").date()\n",
    "\n",
    "    \n",
    "    days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    dayNumber=conver_date.weekday()\n",
    "    return days[dayNumber]\n",
    "\n",
    "\n",
    "material['yoil']=material['SELL_DATE'].map(dow)\n",
    "material.head()\n",
    "\n",
    "nd1=material.loc[material['SELL_DATE']<'2018-12-31',]\n",
    "nd2=material.loc[material['SELL_DATE']>='2018-12-31',]\n",
    "l1=nd1['SELL_DATE'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])\n",
    "l2=nd2['SELL_DATE'].map(lambda x : datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])).isocalendar()[1])+52\n",
    "l3=l1.append(l2)\n",
    "l3\n",
    "material['week_num']=l3+2\n",
    "material.tail(20)\n",
    "material.loc[[0,1,2,3,4],'week_num']=1\n",
    "material.loc[[5,6,7,8],'week_num']=2\n",
    "material.columns\n",
    "material_list=['갈치_price', '감자_price', '계란_price', '고등어_price',\n",
    "       '고춧가루_price', '꽁치_price', '닭고기_price', '당근_price', '대파_price',\n",
    "       '마늘_price', '멸치_price', '목살_price', '무_price', '배_price', '배추_price',\n",
    "       '상추_price', '새우_price', '쌀_price', '양파_price', '우유_price',\n",
    "       '참깨_price',  '팽이버섯_price', '한우양지_price', '호박_price',\n",
    "       '고구마_price', '시금치_price', '오이_price', '오징어_price', '미나리_price',\n",
    "       '깻잎_price', '피망_price']\n",
    "\n",
    "for k in range(31):\n",
    "    material[material_list[k]+'_lag1']=np.repeat(0,len(material))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(9,len(material),1):\n",
    "    for k in range(31):\n",
    "        material.loc[i,material_list[k]+'_lag1']=float(material.loc[material['week_num']==material.loc[i,'week_num']-1,][material_list[k]].mean())\n",
    "    \n",
    "material.head()\n",
    "material=material[[material_list[k]+'_lag1' for k in range(31)]+['SELL_DATE']]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "check_idx=meal2_add['CUSTOMER_ID'].map(lambda x: x in list(meal2_init['CUSTOMER_ID']))\n",
    "check_idx.sum()\n",
    "\n",
    "meal2_add=meal2_add.loc[~check_idx,]\n",
    "\n",
    "meal2=pd.concat([meal2_init,meal2_add])\n",
    "meal1=pd.concat([meal1_init,meal1_add])\n",
    "\n",
    "meal1=meal1.reset_index()\n",
    "meal1=meal1.drop(\"index\",axis=1)\n",
    "\n",
    "meal2=meal2.reset_index()\n",
    "meal2=meal2.drop(\"index\",axis=1)\n",
    "\n",
    "a1=meal1\n",
    "a2=meal2\n",
    "\n",
    "a1.tail(70)\n",
    "a2.head(50)\n",
    "a3=a1.groupby('CUSTOMER_ID')['QUANTITY'].sum()\n",
    "a3=a3.reset_index()\n",
    "a3.head()\n",
    "\n",
    "##\n",
    "df1=pd.read_csv(\"total_brand1.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"total_brand2.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"total_brand3.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"total_brand4.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"total_brand5.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"total_brand6.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"total_brand7.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"total_brand8.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"total_brand9.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"total_brand10.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"total_brand11.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"total_brand12.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"total_brand13.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "\n",
    "\n",
    "i=6\n",
    "    \n",
    "\n",
    "if i ==0:\n",
    "    brandname=\"Chef`sCounter\"\n",
    "elif i==1:\n",
    "    brandname=\"우리미각면\"\n",
    "elif i==2:\n",
    "    brandname=\"나폴리폴리\"\n",
    "elif i==3:\n",
    "    brandname=\"TakeOut\"\n",
    "elif i==4:\n",
    "    brandname=\"KOREAN1\"\n",
    "elif i==5:\n",
    "    brandname=\"싱푸차이나\"\n",
    "elif i==6:\n",
    "    brandname=\"KOREAN2\"\n",
    "elif i==7:\n",
    "    brandname=\"Western\"\n",
    "elif i==8:\n",
    "    brandname=\"고슬고슬비빈\"\n",
    "elif i==9:\n",
    "    brandname=\"아시안픽스\"\n",
    "elif i==10:\n",
    "    brandname=\"스냅스낵\"\n",
    "elif i==11:\n",
    "    brandname=\"가츠엔\"\n",
    "else:\n",
    "    brandname=\"탕맛기픈\"\n",
    "    \n",
    "\n",
    "\n",
    "new_brand1=df_list[i]\n",
    "new_brand1=new_brand1.fillna(0)\n",
    "new_brand1['y_g12']=new_brand1['y_g1']+new_brand1['y_g2']\n",
    "new_brand1['y_g34']=new_brand1['y_g3']+new_brand1['y_g4']\n",
    "\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31','2019-06-07'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "new_brand1=pd.merge(new_brand1,material,how='left',on='SELL_DATE')\n",
    "\n",
    "\n",
    "notnan_idx=set(new_brand1.loc[new_brand1['감자_price_lag1']>0,].index)\n",
    "nan_idx=list(set(new_brand1.index) - notnan_idx)\n",
    "\n",
    "for j in nan_idx:\n",
    "    for k in range(31):\n",
    "        new_brand1.loc[j,material_list[k]+'_lag1']=new_brand1.loc[j-1,material_list[k]+'_lag1']\n",
    "\n",
    "\n",
    "new_brand1.columns\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "weather_predict=pd.read_csv(\"날씨예측결과.csv\",engine='python')\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['SELL_DATE', 'humidity', 'rain', 'temp', 'wind', 'C1_Monday', 'C1_Saturday',\n",
    "       'C1_Thursday', 'C1_Tuesday', 'C1_Wednesday', 'D1_2', 'D1_3', 'D1_4',\n",
    "       'D1_5', 'D1_6', 'D1_7', 'D1_8', 'D1_9', 'D1_10', 'D1_11', 'D1_12',\n",
    "       'E1_True', 'F1_True', 'y', 'holiday1', 'holiday2', 'holiday3',\n",
    "       '갈치_price_lag1', '감자_price_lag1', '계란_price_lag1', '고등어_price_lag1',\n",
    "       '고춧가루_price_lag1', '꽁치_price_lag1', '닭고기_price_lag1', '당근_price_lag1',\n",
    "       '대파_price_lag1', '마늘_price_lag1', '멸치_price_lag1', '목살_price_lag1',\n",
    "       '무_price_lag1', '배_price_lag1', '배추_price_lag1', '상추_price_lag1',\n",
    "       '새우_price_lag1', '쌀_price_lag1', '양파_price_lag1', '우유_price_lag1',\n",
    "       '참깨_price_lag1', '팽이버섯_price_lag1', '한우양지_price_lag1', '호박_price_lag1',\n",
    "       '고구마_price_lag1', '시금치_price_lag1', '오이_price_lag1', '오징어_price_lag1',\n",
    "       '미나리_price_lag1', '깻잎_price_lag1', '피망_price_lag1', 'y_g12',\n",
    "       'y_g34','y_w','y_m']]\n",
    "else:\n",
    "    new_brand1=new_brand1[['SELL_DATE', 'humidity', 'rain', 'temp', 'wind', 'C1_Monday',\n",
    "       'C1_Thursday', 'C1_Tuesday', 'C1_Wednesday', 'D1_2', 'D1_3', 'D1_4',\n",
    "       'D1_5', 'D1_6', 'D1_7', 'D1_8', 'D1_9', 'D1_10', 'D1_11', 'D1_12',\n",
    "       'E1_True', 'F1_True', 'y', 'holiday1', 'holiday2', 'holiday3',\n",
    "       '갈치_price_lag1', '감자_price_lag1', '계란_price_lag1', '고등어_price_lag1',\n",
    "       '고춧가루_price_lag1', '꽁치_price_lag1', '닭고기_price_lag1', '당근_price_lag1',\n",
    "       '대파_price_lag1', '마늘_price_lag1', '멸치_price_lag1', '목살_price_lag1',\n",
    "       '무_price_lag1', '배_price_lag1', '배추_price_lag1', '상추_price_lag1',\n",
    "       '새우_price_lag1', '쌀_price_lag1', '양파_price_lag1', '우유_price_lag1',\n",
    "       '참깨_price_lag1', '팽이버섯_price_lag1', '한우양지_price_lag1', '호박_price_lag1',\n",
    "       '고구마_price_lag1', '시금치_price_lag1', '오이_price_lag1', '오징어_price_lag1',\n",
    "       '미나리_price_lag1', '깻잎_price_lag1', '피망_price_lag1', 'y_g12',\n",
    "       'y_g34','y_w','y_m']]\n",
    "\n",
    "new_brand1.columns\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train1=train['y_w']\n",
    "X_train=train.drop(['y_w','y_m','y_g12','y_g34','y','SELL_DATE'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y_w','y_m','y_g12','y_g34','y','SELL_DATE'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train1[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train1,X_train)\n",
    "results = model.fit()\n",
    "pred21=results.predict(X_test) # for y_g12\n",
    "\n",
    "y_train2=train['y_m']\n",
    "X_train=train.drop(['y_w','y_m','y_g12','y_g34','y','SELL_DATE'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y_w','y_m','y_g12','y_g34','y','SELL_DATE'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train2[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train2,X_train)\n",
    "results = model.fit()\n",
    "pred22=results.predict(X_test) # for y_g34    \n",
    "\n",
    "pred2=pred21+pred22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 10 예측모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_predict=pd.read_csv(\"날씨예측결과.csv\",engine='python')\n",
    "weather_predict.tail()\n",
    "\n",
    "df1=pd.read_csv(\"total_brand1.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"total_brand2.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"total_brand3.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"total_brand4.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"total_brand5.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"total_brand6.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"total_brand7.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"total_brand8.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"total_brand9.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"total_brand10.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"total_brand11.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"total_brand12.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"total_brand13.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "list6=[]\n",
    "list7=[]\n",
    "list8=[]\n",
    "list9=[]\n",
    "list10=[]\n",
    "list11=[]\n",
    "list12=[]\n",
    "list13=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=8\n",
    "new_brand1=df_list[i]\n",
    "new_brand1=new_brand1.fillna(0)\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12', 'C1_Saturday', 'C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "else:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train=train['y']\n",
    "X_train=train.drop(['y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "Y = y_train[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test4[variables]\n",
    "\n",
    "model= sm.OLS(y_train,X_train)\n",
    "results = model.fit()\n",
    "pred2=results.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 12 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_predict=pd.read_csv(\"날씨예측결과.csv\",engine='python')\n",
    "weather_predict.tail()\n",
    "\n",
    "df1=pd.read_csv(\"total_brand1.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"total_brand2.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"total_brand3.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"total_brand4.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"total_brand5.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"total_brand6.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"total_brand7.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"total_brand8.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"total_brand9.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"total_brand10.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"total_brand11.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"total_brand12.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"total_brand13.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "\n",
    "\n",
    "\n",
    "i=8\n",
    "new_brand1=df_list[i]\n",
    "new_brand1=new_brand1.fillna(0)\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12', 'C1_Saturday','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','SELL_DATE']]\n",
    "else:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','SELL_DATE']]\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train=train['y']\n",
    "X_train=train.drop(['y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "\n",
    "\n",
    "Y = y_train[:]\n",
    "X = X_train[:]\n",
    "\n",
    "k = 659\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = sm.OLS(Y,X)\n",
    "    results = model_k.fit()\n",
    "    \n",
    "    \n",
    "        #create instance of influence\n",
    "    influence = results.get_influence()\n",
    "    \n",
    "    #leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    residual=Y-results.predict()\n",
    "    resid_p=residual/(1-leverage) #28.172367\n",
    "    resid_pp=resid_p**2\n",
    "    press=resid_pp.sum()\n",
    "    return press\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "PRESS_list = [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "best_PRESS = np.inf\n",
    "for i in range(1,k+1):\n",
    "    \n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            PRESS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "            #print(list(combo) + features)\n",
    "            if PRESS < best_PRESS:\n",
    "                best_PRESS = PRESS\n",
    "                best_feature = combo[0]\n",
    "    if best_feature in features:\n",
    "        break\n",
    "    else:\n",
    "        features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)    \n",
    "        PRESS_list.append(best_PRESS)\n",
    "        features_list[i] = features.copy()\n",
    "    \n",
    "len(features_list)\n",
    "variables=features_list[len(features_list)]\n",
    "len(variables)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#modeling1 (random forest) 7 \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train=train[variables]\n",
    "X_test=X_test[variables]\n",
    "\n",
    "model= sm.OLS(y_train,X_train)\n",
    "results = model.fit()\n",
    "pred2=results.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 13 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab5f19be386d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#3.기본+기상4(예측) 배깅\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mweather_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"날씨예측결과.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mweather_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total_brand1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euc-kr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#3.기본+기상4(예측) 배깅\n",
    "weather_predict=pd.read_csv(\"날씨예측결과.csv\",engine='python')\n",
    "weather_predict.tail()\n",
    "\n",
    "df1=pd.read_csv(\"total_brand1.csv\",encoding='euc-kr')\n",
    "df2=pd.read_csv(\"total_brand2.csv\",encoding='euc-kr')\n",
    "df3=pd.read_csv(\"total_brand3.csv\",encoding='euc-kr')\n",
    "df4=pd.read_csv(\"total_brand4.csv\",encoding='euc-kr')\n",
    "df5=pd.read_csv(\"total_brand5.csv\",encoding='euc-kr')\n",
    "df6=pd.read_csv(\"total_brand6.csv\",encoding='euc-kr')\n",
    "df7=pd.read_csv(\"total_brand7.csv\",encoding='euc-kr')\n",
    "df8=pd.read_csv(\"total_brand8.csv\",encoding='euc-kr')\n",
    "df9=pd.read_csv(\"total_brand9.csv\",encoding='euc-kr')\n",
    "df10=pd.read_csv(\"total_brand10.csv\",encoding='euc-kr')\n",
    "df11=pd.read_csv(\"total_brand11.csv\",encoding='euc-kr')\n",
    "df12=pd.read_csv(\"total_brand12.csv\",encoding='euc-kr')\n",
    "df13=pd.read_csv(\"total_brand13.csv\",encoding='euc-kr')\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13]\n",
    "\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "list6=[]\n",
    "list7=[]\n",
    "list8=[]\n",
    "list9=[]\n",
    "list10=[]\n",
    "list11=[]\n",
    "list12=[]\n",
    "list13=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=6\n",
    "new_brand1=df_list[i]\n",
    "new_brand1=new_brand1.fillna(0)\n",
    "new_brand1['holiday1']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday2']=np.repeat(0,len(new_brand1))\n",
    "new_brand1['holiday3']=np.repeat(0,len(new_brand1))\n",
    "\n",
    "holi_idx1=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-02-14'])\n",
    "holi_idx2=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-03-02','2018-04-30','2018-05-21','2018-10-08','2018-12-24','2018-12-31'])\n",
    "holi_idx3=new_brand1['SELL_DATE'].map(lambda x : x in ['2018-05-04','2018-09-21','2018-09-27','2018-08-03','2019-05-03','2019-02-01','2018-02-07','2019-02-28','2018-09-28','2019-02-08'])\n",
    "\n",
    "new_brand1.loc[holi_idx1,'holiday1']=1\n",
    "new_brand1.loc[holi_idx2,'holiday2']=1\n",
    "new_brand1.loc[holi_idx3,'holiday3']=1\n",
    "\n",
    "idx=new_brand1.loc[new_brand1['SELL_DATE']=='2018-10-25',].index[0] #아웃라이어 취급\n",
    "dfdf=new_brand1.drop(idx)\n",
    "new_brand1=dfdf\n",
    "\n",
    "\n",
    "#modeling\n",
    "if i==6:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12', 'C1_Saturday'  ,  'C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "else:\n",
    "    new_brand1=new_brand1[['holiday1','holiday2','holiday3','y','y_w','y_m','y_g1','y_g2','y_g3','y_g4','E1_True','F1_True','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7','D1_8','D1_9','D1_10','D1_11','D1_12','C1_Monday','C1_Tuesday','C1_Wednesday','C1_Thursday','humidity','rain','temp','wind','SELL_DATE']]\n",
    "\n",
    "new_brand1['y_g12']=new_brand1['y_g1']+new_brand1['y_g2']\n",
    "new_brand1['y_g34']=new_brand1['y_g3']+new_brand1['y_g4']\n",
    "\n",
    "\n",
    "new_brand1.head()\n",
    "#divide train with test\n",
    "\n",
    "test=new_brand1.loc[new_brand1['SELL_DATE']>'2019-08-16',] \n",
    "test=test.loc[test['SELL_DATE']<='2019-08-23']\n",
    "train=new_brand1.loc[new_brand1['SELL_DATE']<='2019-08-16',]\n",
    "\n",
    "#modeling1 (random forest)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_train1=train['y_w']\n",
    "y_train2=train['y_m']\n",
    "X_train=train.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "X_train.columns\n",
    "\n",
    "y_test=test['y']\n",
    "X_test=test.drop(['y_g12','y_g34','y','SELL_DATE','y_w','y_m','y_g1','y_g2','y_g3','y_g4'],axis=1)\n",
    "\n",
    "pppp=len(X_train.columns)\n",
    "rf=RandomForestRegressor(n_estimators=1000,criterion='mae',n_jobs=3,max_features=pppp)\n",
    "rf.fit(X_train,y_train1)\n",
    "\n",
    "X_test2=X_test.drop(['humidity','rain','temp','wind'],axis=1)\n",
    "X_test2['SELL_DATE']=test['SELL_DATE']\n",
    "X_test3=pd.merge(X_test2,weather_predict,how='left',on='SELL_DATE')\n",
    "X_test4=X_test3.drop('SELL_DATE',axis=1)\n",
    "\n",
    "pred1=rf.predict(X_test4)\n",
    "\n",
    "pppp=len(X_train.columns)\n",
    "rf=RandomForestRegressor(n_estimators=1000,criterion='mae',n_jobs=3,max_features=pppp)\n",
    "rf.fit(X_train,y_train2)\n",
    "\n",
    "pred2=rf.predict(X_test4)    \n",
    "\n",
    "pred=pred1+pred2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
